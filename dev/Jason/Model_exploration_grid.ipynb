{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from scipy.stats import expon as sp_expon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turns off annoying warnings. If there are ever serious data\n",
    "# errors, trying removing this line.\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_Kaggle_file(predict_probabilities, columns, output_file_name=\"auto\", decimal_limit=3):\n",
    "    \"\"\"\n",
    "    Outputs a file that can be submitted to Kaggle. This takes a long time to run, so you \n",
    "    shouldn't run it that often. Instead, just have good internal validation techniques so you\n",
    "    don't have to check the public leaderboard.\n",
    "    \n",
    "    Required imports: \n",
    "    import time\n",
    "    import pandas as pd\n",
    "    \n",
    "    predict_probabilities: array-like of shape = [n_samples, n_classes]. Is the output of a \n",
    "        predict_proba method of a sklearn classifier\n",
    "        \n",
    "    columns: array or list of column names that are in the same order as the columns of the \n",
    "        predict_probabilities method. If LabelEncoder was used, is accessed via the classes_ \n",
    "        attribute. Don't include an \"Id\" column.\n",
    "        \n",
    "    output_file_name: If \"auto\" names it sf_crime_test_predictions_<YearMonthDay-HourMinuteSecond>, \n",
    "        else uses the string entered as the file name.\n",
    "        \n",
    "    decimal_limit: If None uses full precision, else formats predictions based on that precision. \n",
    "        Can significantly reduce the filesize and make writing the file faster.\n",
    "        i.e. actual prediction = .2352452435, decimal_limit=2 --> .24, decimal_limit=3 --> .235, etc.\n",
    "    \"\"\"\n",
    "    predictions = pd.DataFrame(predict_probabilities, columns=columns)\n",
    "    predictions.index.name = \"Id\"\n",
    "    if output_file_name == \"auto\":\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        output_file_name = \"sf_crime_test_predictions_\" + timestr + \".csv\"\n",
    "    if decimal_limit:\n",
    "        decimal_limit = '%%.%df' % decimal_limit\n",
    "    predictions.to_csv(output_file_name, float_format=decimal_limit)\n",
    "    print(\"Finished writing file: \", output_file_name)\n",
    "    \n",
    "def gridsearch_parameter_results(gridsearch):\n",
    "    \"\"\"\n",
    "    Evaluate how each feature impacts the final prediction.\n",
    "    \"\"\"\n",
    "    scores = [val[1] for val in gridsearch.grid_scores_]\n",
    "    params = gridsearch.param_distributions.keys()\n",
    "    for param in params:\n",
    "        arg_values = [val[0][param] for val in gridsearch.grid_scores_]\n",
    "        no_strings = all([type(arg) != str for arg in arg_values])\n",
    "        if no_strings:\n",
    "            plt.scatter(arg_values, scores, linewidth=0, s=100, alpha=.25)\n",
    "            plt.title(param)\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Make all arg values strings\n",
    "            arg_values = [str(arg) for arg in arg_values]\n",
    "            results = pd.Series(data=scores, index=arg_values)\n",
    "            group_means = results.groupby(level=0).mean()\n",
    "            group_means.plot(kind=\"bar\", title=param, rot=0, linewidth=0, colormap=\"viridis\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class BasicFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_features=True, DayOfWeek_features=True, PdDistrict_features=True, Address_features=True):\n",
    "        self.date_features = date_features\n",
    "        self.DayOfWeek_features = DayOfWeek_features\n",
    "        self.PdDistrict_features = PdDistrict_features\n",
    "        self.Address_features = Address_features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_df, y=None):\n",
    "        # Features from dates\n",
    "        X = X_df.copy()\n",
    "        \n",
    "        if self.date_features:\n",
    "            X['Year'] = X.Dates.apply(lambda x: int(x[:4])) # Hypothesis: The distribution of crimes changed over time\n",
    "            X['Month'] = X.Dates.apply(lambda x: int(x[5:7])) # H: Certain crimes occur during some months more than others\n",
    "            X['Hour'] = X.Dates.apply(lambda x: int(x[11:13])) # H: Certain crimes occur at day, others at night\n",
    "            X['Minute'] = X.Dates.apply(lambda x: int(x[14:16])) # H: Certain crimes are rounded to the nearest hour\n",
    "            # Idea: Is holiday feature. H: Holidays --> Tourists --> Different types of crimes\n",
    "\n",
    "        # Features from DayOfWeek\n",
    "        if self.DayOfWeek_features:\n",
    "            X['DayOfWeekNum'] = X[\"DayOfWeek\"].map({\"Tuesday\":0, \"Wednesday\":1, \n",
    "                                                 \"Thursday\":2, \"Friday\":3, \n",
    "                                                 \"Saturday\":4, \"Sunday\":5, \n",
    "                                                 \"Monday\":6}) # H: Different days have different crime distributions\n",
    "            X['IsWeekend'] = X[\"DayOfWeekNum\"].apply(lambda x: 1*((x == 4) | (x == 5))) # H: Weekends are special\n",
    "\n",
    "        # Features from PdDistrict\n",
    "        if self.PdDistrict_features:\n",
    "            X['PdDistrictNum'] = LabelEncoder().fit_transform(X.PdDistrict) # H: Different districts have different crimes\n",
    "\n",
    "        # Features from Address\n",
    "        if self.Address_features:\n",
    "            X['Intersection'] = X.Address.apply(lambda x: 1*(\"/\" in x)) # H: Intersections have unique crimes\n",
    "        \n",
    "        # Idea: Make categorical feature of all addresses based on number of crimes at the address\n",
    "        # Idea: Make categorical feature of certain popular streets\n",
    "\n",
    "        # Features from X & Y\n",
    "        # Idea: Make a feature that corresponds to whether the crime was near the ocean. \n",
    "\n",
    "        # Other ideas:\n",
    "        # Certain crimes result in multiple observations (for example the first and second observation in \n",
    "        # the dataset are located) at the same location and occur at the same time. The crimes, warrant arrest \n",
    "        # and traffic violation arrest, seem to go with each other. \n",
    "        #    Specific feature ideas: Number of observations associated with crime. In this case, the value would be 2.\n",
    "        #    Specific feature ideas: If these crimes are split between the training and test datasets, perhaps the crimes\n",
    "        #        in the training data set would inform the crime in the test data set.\n",
    "        return X\n",
    "    \n",
    "\n",
    "class AddressCrimeCounts(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    For each address, adds a feature based on the number of crimes that occurred at the address\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "\n",
    "class PopularStreetCounts(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Categorical feature for popular streets\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class NearOcean(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    For each crime, measure distance to nearest beach\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X    \n",
    "\n",
    "    \n",
    "class DuplicateCrimeCounts(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    For each crime, count the number of other crimes that occurred at the exact time and location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        result = X.groupby([\"Dates\", \"X\", \"Y\"]).size()\n",
    "        result = result.reset_index(name=\"crime_count\")\n",
    "        X = X.merge(result, how=\"left\", on=[\"Dates\", \"X\", \"Y\"])\n",
    "        return X\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class JustNumerics(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Drops all columns that are objects\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.numeric_columns = X.dtypes[X.dtypes != \"object\"].index\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.numeric_columns]\n",
    "    \n",
    "    \n",
    "class PCATransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    PCA with an argument that allows the user to skip the transform \n",
    "    altogether.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=.1, skip=False, whiten=False, standard_scalar=True):\n",
    "        self.n_components = n_components\n",
    "        self.skip = skip\n",
    "        self.whiten = whiten\n",
    "        self.standard_scalar = standard_scalar\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if not self.skip:\n",
    "            if self.standard_scalar:\n",
    "                self.std_scalar = StandardScaler().fit(X)\n",
    "                X = self.std_scalar.transform(X)\n",
    "            self.pca = PCA(n_components=self.n_components, whiten=self.whiten).fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if not self.skip:\n",
    "            if self.standard_scalar:\n",
    "                X = self.std_scalar.transform(X)\n",
    "            return self.pca.transform(X)\n",
    "        return X\n",
    "    \n",
    "class ModelBasedFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds a feature to the dataset based on the output of a model.\n",
    "    \n",
    "    Should include in FeatureUnion.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, feature_name, skip=False, train_cv=None):\n",
    "        self.model = model\n",
    "        self.feature_name = feature_name\n",
    "        self.skip = skip\n",
    "        self.train_cv = train_cv\n",
    "    \n",
    "    def _get_random_item(self, items):\n",
    "        # Currently only handles scipy distributions\n",
    "        return items.rvs()\n",
    "    \n",
    "    def fit(self, X, y=None, *args, **kwargs):\n",
    "        # Purpose of skip is to skip the estimator\n",
    "        if self.skip:\n",
    "            return self\n",
    "        \n",
    "        # Hash train data. If test data equals train data, \n",
    "        # use cv predictions.\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.hashed_value = hash(X.values.data.tobytes())\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            self.hashed_value = hash(X.data.tobytes())\n",
    "        else:\n",
    "            print(\"Can't hash data\")\n",
    "            \n",
    "        # Get specific model param combo for this iteration\n",
    "        self.model_params = {key: self._get_random_item(kwargs[key]) for key in kwargs}\n",
    "        \n",
    "        # Set params of model to these parameters\n",
    "        self.model.set_params(**self.model_params)\n",
    "        \n",
    "        # Fit model\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "        # Save y values\n",
    "        self.y = y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Purpose of skip is to skip the estimator\n",
    "        if self.skip:\n",
    "            return X\n",
    "        \n",
    "        # Is the data being transformed the same as the training data\n",
    "        is_train_data = False\n",
    "        if isinstance(X, pd.DataFrame) and self.hashed_value == hash(X.values.data.tobytes()):\n",
    "            is_train_data = True\n",
    "        if isinstance(X, np.ndarray) and self.hashed_value == hash(X.data.tobytes()):\n",
    "            is_train_data = True\n",
    "        \n",
    "        # If the dataset is the training data, use CV predictions\n",
    "        if is_train_data:\n",
    "            feature = cross_val_predict(clone(self.model), X, self.y)#, cv=self.train_cv)\n",
    "            \n",
    "        # Otherwise, use the model to predict\n",
    "        else:\n",
    "            feature = self.model.predict(X)\n",
    "        \n",
    "        # Add feature to dataset\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X[self.feature_name] = feature\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = np.c_[X, feature]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_of_data_to_use = 1\n",
    "\n",
    "# Read in training data\n",
    "X = pd.read_csv(\"../../data/train.csv\")\n",
    "\n",
    "X = DuplicateCrimeCounts().fit_transform(X)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X = X.sample(frac=percent_of_data_to_use, random_state=42).reset_index(drop=True)\n",
    "y = X.pop('Category')\n",
    "\n",
    "# Convert y labels to integer representations\n",
    "labels = LabelEncoder()\n",
    "y = labels.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This seed is required for each gridsearch to be identical\n",
    "np.random.seed(1)\n",
    "\n",
    "# Define the ML pipe\n",
    "pipe = Pipeline([(\"BFE\", BasicFeatures()), \n",
    "                 (\"just_numerics\", JustNumerics()), \n",
    "#                  (\"PCA\", PCATransform()), \n",
    "                 (\"LR_feature\", ModelBasedFeatures(LogisticRegression(solver='lbfgs'), feature_name=\"LR_feature\", train_cv=2)),\n",
    "#                  (\"RF_feature\", ModelBasedFeatures(model=RandomForestClassifier(), feature_name=\"RF_feature\", train_cv=2)),\n",
    "                 (\"RF\", RandomForestClassifier(n_jobs=-1))])\n",
    "\n",
    "PCA_factor = .6\n",
    "\n",
    "params = {\n",
    "#           \"BFE__date_features\": sp_randint(0, 2),\n",
    "#           \"BFE__DayOfWeek_features\": sp_randint(0, 2),\n",
    "#           \"BFE__PdDistrict_features\": sp_randint(0, 2),\n",
    "#           \"BFE__Address_features\": sp_randint(0, 2),\n",
    "#           \"PCA__n_components\": sp_uniform(PCA_factor, 1-PCA_factor),\n",
    "#           \"PCA__skip\": sp_randint(0, 2),\n",
    "#           \"PCA__standard_scalar\": sp_randint(0, 2),\n",
    "#           \"RF_feature__skip\": sp_randint(0, 2),\n",
    "#           \"LR_feature__skip\": sp_randint(0, 2),\n",
    "          \"RF__n_estimators\": sp_randint(65, 100),\n",
    "          \"RF__max_depth\": sp_randint(15, 50),\n",
    "          \"RF__min_samples_split\": sp_randint(50, 250),\n",
    "          \"RF__min_samples_leaf\": sp_randint(1, 50),\n",
    "          \"RF__max_leaf_nodes\": sp_randint(2500, 3500)\n",
    "         }\n",
    "\n",
    "fit_params = {\n",
    "#               \"RF_feature__n_estimators\": sp_randint(5, 10),\n",
    "#               \"RF_feature__max_depth\": sp_randint(5, 10),\n",
    "              \"LR_feature__C\": sp_expon(4, 50)\n",
    "             }\n",
    " \n",
    "gridsearch = RandomizedSearchCV(pipe, params, fit_params=fit_params, n_iter=10, scoring=\"log_loss\", cv=3, n_jobs=-1)\n",
    "gridsearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(log_loss, greater_is_better=False, needs_proba=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ca1a7c3ab07d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgridsearch_parameter_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgridsearch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "print(\"Best score:\", gridsearch.best_score_, \"\\n\")\n",
    "print(\"\")\n",
    "\n",
    "gridsearch_parameter_results(gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data to predict and make predictions\n",
    "X_predict = pd.read_csv(\"../../data/test.csv\")\n",
    "X_predict = DuplicateCrimeCounts().fit_transform(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_predictions = gridsearch.predict_proba(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Export predictions to file to be submitted to Kaggle\n",
    "make_Kaggle_file(final_predictions, labels.classes_, decimal_limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Export predictions to file to be submitted to Kaggle\n",
    "make_Kaggle_file(final_predictions, labels.classes_, decimal_limit=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
